{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**nn.Sequential和nn.Linear都是nn.Module的子类**]  \n",
    "nn.Sequential定义了一种特殊的Module， 即在PyTorch中表示一个块的类， 它维护了一个由Module组成的有序列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0732, 0.1416, 0.6206, 0.2854, 0.2596, 0.6025, 0.2190, 0.7770, 0.6382,\n",
      "         0.6264, 0.6551, 0.1717, 0.5964, 0.1889, 0.3372, 0.2004, 0.4985, 0.3251,\n",
      "         0.7591, 0.1905],\n",
      "        [0.0231, 0.6089, 0.4982, 0.1456, 0.0419, 0.2254, 0.5718, 0.8016, 0.7208,\n",
      "         0.4983, 0.4431, 0.8389, 0.7194, 0.9320, 0.6088, 0.1000, 0.4035, 0.0782,\n",
      "         0.2163, 0.1977]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6984e-02,  9.1910e-05, -5.3518e-02, -1.7719e-01, -1.5649e-01,\n",
       "          4.3066e-02, -1.4175e-01, -2.5307e-01,  2.3861e-02, -1.2522e-01],\n",
       "        [-1.4696e-02, -7.3726e-02, -8.0044e-02, -1.6672e-01, -1.2479e-01,\n",
       "          1.4736e-02, -2.3628e-01, -3.0702e-01,  1.5554e-01, -1.8936e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(), nn.Linear(256,10))\n",
    "X=torch.rand(2,20)\n",
    "print(X)\n",
    "# 当你像函数一样调用这个层的实例时，Pytorch会自动调用forward函数\n",
    "# 这是因为nn.Module类重载了__call__函数\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继承自Module类的方式\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 调用MLP父类Module的构造函数来进行必要的初始化\n",
    "        super().__init__()\n",
    "        # 声明两个全连接层\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0437,  0.0544,  0.1791, -0.0939, -0.0137, -0.2055, -0.0396,  0.1515,\n",
       "         -0.0580,  0.1183],\n",
       "        [ 0.0301, -0.1073,  0.1183, -0.0726, -0.0579, -0.1767,  0.0094,  0.2949,\n",
       "         -0.0366,  0.0680]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**手动实现Sequential**]  \n",
    "回想一下Sequential的设计是为了把其他模块（module）串起来。  \n",
    "为了构建我们自己的简化的MySequential， 我们只需要定义两个关键函数：  \n",
    "1.一种将块逐个追加到列表中的函数；  \n",
    "2.一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 向pytorch模型module中动态添加模块，模块本身也是一个Module类的实例\n",
    "            # nn.Module类中的_modules是一个OrderedDict(有序字典)，可以保证添加进去的模块的顺序是固定的\n",
    "            # 这里idx是一个int类型，不能直接作为key，需要转换成str类型\n",
    "            self._modules[str(idx)]=module\n",
    "    # 实现前向传播\n",
    "    def forward(self,X):\n",
    "        # OrderedDict保证了按照添加进去的顺序遍历\n",
    "        for block in self._modules.values():\n",
    "            # 链式调用每个模块的forward方法\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1023,  0.0211, -0.0037,  0.0179, -0.3167, -0.0294,  0.0846, -0.0887,\n",
       "          0.0402,  0.0313],\n",
       "        [ 0.0983, -0.0075, -0.0829, -0.0069, -0.4463, -0.1496,  0.0671, -0.0290,\n",
       "         -0.0013,  0.1063]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20,256), nn.ReLU(), nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad = False)\n",
    "        # 在输出之前添加一个全连接层\n",
    "        self.linear = nn.Linear(20,20)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常数参数以及relu函数\n",
    "        X = F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        # 复用全连接层，这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0978, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3464, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20,64), nn.ReLU(),\n",
    "                                 nn.Linear(64,32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32,16)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
